#!/usr/bin/env python3
"""
ML Model A/B Testing Framework
Compare multiple models in production with statistical significance testing
"""

import warnings
warnings.filterwarnings('ignore')

import json
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict

# Statistical testing
try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è  scipy not available - install with: pip install scipy")


@dataclass
class ABTestConfig:
    """A/B test configuration"""
    test_id: str
    model_a: str  # Control model
    model_b: str  # Treatment model
    traffic_split: float = 0.5  # 50/50 split
    min_samples: int = 100  # Minimum samples per variant
    confidence_level: float = 0.95  # 95% confidence
    start_date: str = ""
    end_date: Optional[str] = None
    status: str = "running"  # running, completed, stopped


@dataclass
class ABTestResult:
    """A/B test results"""
    test_id: str
    model_a: str
    model_b: str

    # Metrics for Model A
    a_predictions: int
    a_correct: int
    a_accuracy: float
    a_avg_confidence: float

    # Metrics for Model B
    b_predictions: int
    b_correct: int
    b_accuracy: float
    b_avg_confidence: float

    # Comparison
    accuracy_diff: float  # B - A
    accuracy_diff_pct: float  # (B - A) / A * 100
    is_significant: bool
    p_value: float
    confidence_level: float

    # Recommendation
    winner: str  # 'model_a', 'model_b', 'no_winner'
    recommendation: str

    # Metadata
    test_duration_hours: float
    timestamp: str


class ABTestFramework:
    """
    A/B testing framework for ML models

    Features:
    - Traffic splitting (50/50, 70/30, 90/10, etc.)
    - Statistical significance testing (chi-square, t-test)
    - Real-time metrics tracking
    - Winner determination
    - Gradual rollout support
    """

    def __init__(self, results_dir: str = "/root/AlgoTrendy_v2.6/ml_models/ab_tests"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.active_tests: Dict[str, ABTestConfig] = {}
        self.test_results: Dict[str, List[Dict]] = defaultdict(list)

        self._load_active_tests()

    def _load_active_tests(self):
        """Load active tests from disk"""
        config_file = self.results_dir / "active_tests.json"
        if config_file.exists():
            with open(config_file) as f:
                data = json.load(f)
                for test_data in data.values():
                    config = ABTestConfig(**test_data)
                    self.active_tests[config.test_id] = config

    def _save_active_tests(self):
        """Save active tests to disk"""
        config_file = self.results_dir / "active_tests.json"
        data = {test_id: asdict(config) for test_id, config in self.active_tests.items()}
        with open(config_file, 'w') as f:
            json.dump(data, f, indent=2)

    def create_test(
        self,
        model_a: str,
        model_b: str,
        traffic_split: float = 0.5,
        min_samples: int = 100,
        confidence_level: float = 0.95
    ) -> str:
        """
        Create a new A/B test

        Args:
            model_a: Control model ID
            model_b: Treatment model ID
            traffic_split: Fraction of traffic to model B (0.5 = 50/50 split)
            min_samples: Minimum samples required per variant
            confidence_level: Statistical confidence level (0.95 = 95%)

        Returns:
            Test ID
        """
        test_id = f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        config = ABTestConfig(
            test_id=test_id,
            model_a=model_a,
            model_b=model_b,
            traffic_split=traffic_split,
            min_samples=min_samples,
            confidence_level=confidence_level,
            start_date=datetime.now().isoformat(),
            status="running"
        )

        self.active_tests[test_id] = config
        self._save_active_tests()

        print(f"\n{'='*70}")
        print(f"üß™ A/B TEST CREATED: {test_id}")
        print(f"{'='*70}")
        print(f"  Model A (Control):  {model_a}")
        print(f"  Model B (Treatment): {model_b}")
        print(f"  Traffic Split:      {(1-traffic_split)*100:.0f}% / {traffic_split*100:.0f}%")
        print(f"  Min Samples:        {min_samples} per variant")
        print(f"  Confidence Level:   {confidence_level*100:.0f}%")
        print(f"  Started:            {config.start_date}")
        print(f"{'='*70}\n")

        return test_id

    def assign_model(self, test_id: str, user_id: Optional[str] = None) -> str:
        """
        Assign a model variant to a prediction request

        Args:
            test_id: Active test ID
            user_id: Optional user ID for consistent assignment

        Returns:
            Model ID to use ('model_a' or 'model_b')
        """
        if test_id not in self.active_tests:
            raise ValueError(f"Test {test_id} not found")

        config = self.active_tests[test_id]

        # Consistent assignment for same user
        if user_id:
            import hashlib
            hash_val = int(hashlib.md5(f"{test_id}:{user_id}".encode()).hexdigest(), 16)
            return config.model_b if (hash_val % 100) / 100 < config.traffic_split else config.model_a

        # Random assignment
        return config.model_b if np.random.random() < config.traffic_split else config.model_a

    def record_prediction(
        self,
        test_id: str,
        model_id: str,
        prediction: int,
        actual: Optional[int] = None,
        confidence: float = 0.5
    ):
        """
        Record a prediction result

        Args:
            test_id: Test ID
            model_id: Model used (model_a or model_b)
            prediction: Model prediction (0 or 1)
            actual: Actual outcome (if known)
            confidence: Prediction confidence score
        """
        if test_id not in self.active_tests:
            return

        result = {
            'timestamp': datetime.now().isoformat(),
            'model': model_id,
            'prediction': prediction,
            'actual': actual,
            'correct': actual == prediction if actual is not None else None,
            'confidence': confidence
        }

        self.test_results[test_id].append(result)

        # Persist to disk periodically
        if len(self.test_results[test_id]) % 10 == 0:
            self._save_test_results(test_id)

    def _save_test_results(self, test_id: str):
        """Save test results to disk"""
        results_file = self.results_dir / f"{test_id}_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.test_results[test_id], f, indent=2)

    def analyze_test(self, test_id: str) -> Optional[ABTestResult]:
        """
        Analyze test results and determine winner

        Args:
            test_id: Test ID to analyze

        Returns:
            ABTestResult with detailed analysis
        """
        if test_id not in self.active_tests:
            print(f"‚ùå Test {test_id} not found")
            return None

        config = self.active_tests[test_id]
        results = self.test_results.get(test_id, [])

        if not results:
            print(f"‚ùå No results for test {test_id}")
            return None

        # Filter results with known outcomes
        results_with_outcomes = [r for r in results if r['actual'] is not None]

        if not results_with_outcomes:
            print(f"‚ö†Ô∏è  No results with actual outcomes yet")
            return None

        # Split by model
        a_results = [r for r in results_with_outcomes if r['model'] == config.model_a]
        b_results = [r for r in results_with_outcomes if r['model'] == config.model_b]

        if len(a_results) < config.min_samples or len(b_results) < config.min_samples:
            print(f"‚ö†Ô∏è  Insufficient samples: A={len(a_results)}, B={len(b_results)} (need {config.min_samples} each)")
            return None

        # Calculate metrics
        a_correct = sum(1 for r in a_results if r['correct'])
        a_accuracy = a_correct / len(a_results)
        a_avg_conf = np.mean([r['confidence'] for r in a_results])

        b_correct = sum(1 for r in b_results if r['correct'])
        b_accuracy = b_correct / len(b_results)
        b_avg_conf = np.mean([r['confidence'] for r in b_results])

        # Statistical significance test
        is_significant, p_value = self._test_significance(a_results, b_results, config.confidence_level)

        # Determine winner
        accuracy_diff = b_accuracy - a_accuracy
        accuracy_diff_pct = (accuracy_diff / a_accuracy * 100) if a_accuracy > 0 else 0

        if is_significant:
            if accuracy_diff > 0:
                winner = 'model_b'
                recommendation = f"‚úÖ Deploy {config.model_b} (significantly better)"
            else:
                winner = 'model_a'
                recommendation = f"‚úÖ Keep {config.model_a} (significantly better)"
        else:
            winner = 'no_winner'
            recommendation = f"‚ûñ No significant difference - keep {config.model_a}"

        # Test duration
        start = datetime.fromisoformat(config.start_date)
        duration = (datetime.now() - start).total_seconds() / 3600  # hours

        result = ABTestResult(
            test_id=test_id,
            model_a=config.model_a,
            model_b=config.model_b,
            a_predictions=len(a_results),
            a_correct=a_correct,
            a_accuracy=a_accuracy,
            a_avg_confidence=float(a_avg_conf),
            b_predictions=len(b_results),
            b_correct=b_correct,
            b_accuracy=b_accuracy,
            b_avg_confidence=float(b_avg_conf),
            accuracy_diff=float(accuracy_diff),
            accuracy_diff_pct=float(accuracy_diff_pct),
            is_significant=is_significant,
            p_value=float(p_value),
            confidence_level=config.confidence_level,
            winner=winner,
            recommendation=recommendation,
            test_duration_hours=float(duration),
            timestamp=datetime.now().isoformat()
        )

        # Save analysis
        analysis_file = self.results_dir / f"{test_id}_analysis.json"
        with open(analysis_file, 'w') as f:
            json.dump(asdict(result), f, indent=2)

        # Print report
        self._print_analysis(result, config)

        return result

    def _test_significance(
        self,
        a_results: List[Dict],
        b_results: List[Dict],
        confidence_level: float
    ) -> Tuple[bool, float]:
        """Test statistical significance using chi-square test"""
        if not SCIPY_AVAILABLE:
            # Fallback: simple comparison with large sample assumption
            a_acc = sum(1 for r in a_results if r['correct']) / len(a_results)
            b_acc = sum(1 for r in b_results if r['correct']) / len(b_results)
            diff = abs(b_acc - a_acc)
            # Rough heuristic: >5% difference with >100 samples is significant
            is_sig = diff > 0.05 and len(a_results) > 100 and len(b_results) > 100
            return is_sig, 0.05

        # Chi-square test
        a_correct = sum(1 for r in a_results if r['correct'])
        a_incorrect = len(a_results) - a_correct
        b_correct = sum(1 for r in b_results if r['correct'])
        b_incorrect = len(b_results) - b_correct

        observed = np.array([[a_correct, a_incorrect], [b_correct, b_incorrect]])
        chi2, p_value, dof, expected = stats.chi2_contingency(observed)

        is_significant = p_value < (1 - confidence_level)

        return is_significant, p_value

    def _print_analysis(self, result: ABTestResult, config: ABTestConfig):
        """Print formatted analysis report"""
        print("\n" + "="*70)
        print(f"üìä A/B TEST ANALYSIS: {result.test_id}")
        print("="*70)

        print(f"\nüìÖ Test Duration: {result.test_duration_hours:.1f} hours")
        print(f"üìä Confidence Level: {result.confidence_level*100:.0f}%")

        print(f"\nüîµ Model A ({result.model_a}):")
        print(f"   Predictions:  {result.a_predictions:,}")
        print(f"   Correct:      {result.a_correct:,}")
        print(f"   Accuracy:     {result.a_accuracy*100:.2f}%")
        print(f"   Avg Confidence: {result.a_avg_confidence:.3f}")

        print(f"\nüî¥ Model B ({result.model_b}):")
        print(f"   Predictions:  {result.b_predictions:,}")
        print(f"   Correct:      {result.b_correct:,}")
        print(f"   Accuracy:     {result.b_accuracy*100:.2f}%")
        print(f"   Avg Confidence: {result.b_avg_confidence:.3f}")

        print(f"\nüìà Comparison:")
        print(f"   Accuracy Difference: {result.accuracy_diff*100:+.2f}% ({result.accuracy_diff_pct:+.1f}%)")
        print(f"   P-value: {result.p_value:.4f}")
        print(f"   Statistically Significant: {'‚úÖ YES' if result.is_significant else '‚ùå NO'}")

        print(f"\nüèÜ Winner: {result.winner.upper()}")
        print(f"üí° Recommendation: {result.recommendation}")

        print("\n" + "="*70 + "\n")

    def stop_test(self, test_id: str, reason: str = "Manual stop"):
        """Stop an active test"""
        if test_id in self.active_tests:
            self.active_tests[test_id].status = "stopped"
            self.active_tests[test_id].end_date = datetime.now().isoformat()
            self._save_active_tests()
            print(f"üõë Test {test_id} stopped: {reason}")

    def list_tests(self) -> List[ABTestConfig]:
        """List all active tests"""
        return list(self.active_tests.values())


# Example usage
if __name__ == "__main__":
    print("\n" + "="*70)
    print("üß™ ML A/B TESTING FRAMEWORK")
    print("="*70)

    print("\nFeatures:")
    print("- Traffic splitting (50/50, 70/30, 90/10, etc.)")
    print("- Statistical significance testing (chi-square)")
    print("- Real-time metrics tracking")
    print("- Winner determination with confidence")
    print("- Gradual rollout support")

    print("\nUsage Example:")
    print("```python")
    print("# Create test")
    print("framework = ABTestFramework()")
    print("test_id = framework.create_test(")
    print("    model_a='20251020_223413',  # Control")
    print("    model_b='20251021_120000',  # Treatment")
    print("    traffic_split=0.5,          # 50/50 split")
    print("    min_samples=100")
    print(")")
    print("")
    print("# In production code:")
    print("model_to_use = framework.assign_model(test_id, user_id='user123')")
    print("prediction = model.predict(X)")
    print("framework.record_prediction(test_id, model_to_use, prediction, actual, confidence)")
    print("")
    print("# Analyze results:")
    print("result = framework.analyze_test(test_id)")
    print("```")
    print("="*70 + "\n")
