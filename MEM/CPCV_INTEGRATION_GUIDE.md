# CPCV Integration Guide for MEM Training Pipeline

**Created**: October 21, 2025
**Status**: Ready for Integration
**Priority**: HIGH - Critical for production deployment

---

## üéØ Overview

This guide shows how to integrate Combinatorial Purged Cross-Validation (CPCV) into the existing MEM model training pipeline to prevent overfitting.

### Why This Matters

**Current Risk**: Simple train/test split has ~25% false discovery rate (research-backed)
**With CPCV**: False discovery rate < 10%
**Impact**: More reliable production performance, fewer failed strategies

---

## üìä Current vs CPCV Validation

### Current Approach (Simple Split)
```python
# Current: retrain_model.py (RISKY - Overfitting prone)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    features, labels,
    test_size=0.2,
    random_state=42
)

model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)
```

**Problems**:
- ‚ùå Single test set (could be lucky)
- ‚ùå No temporal awareness (data leakage risk)
- ‚ùå No stability testing (high variance = overfitting)
- ‚ùå Random split ignores time series nature

### New Approach (CPCV - Research-Backed)
```python
# New: Using CPCV (ROBUST - Prevents overfitting)
from cpcv_validator import CombinatorialPurgedCrossValidator, CPCVConfig

# Configure CPCV
config = CPCVConfig(
    n_splits=5,           # 5 different train/test combinations
    embargo_pct=0.01,     # 1% embargo period (prevents leakage)
    test_size=0.2,        # 20% test set
    min_train_size=0.5    # Minimum 50% training data
)

validator = CombinatorialPurgedCrossValidator(config)

# Validate across multiple folds
fold_scores = []
for fold in validator.split(features, labels, timestamps):
    X_train = features.iloc[fold.train_indices]
    X_test = features.iloc[fold.test_indices]
    y_train = labels.iloc[fold.train_indices]
    y_test = labels.iloc[fold.test_indices]

    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    fold_scores.append(score)

    # Store metrics
    fold.performance_metrics = {'accuracy': score}

# Calculate stability
stability = validator.calculate_stability_metrics()

# Decision logic
if stability['coefficient_of_variation'] < 0.15:
    print("‚úÖ STABLE - Safe to deploy")
    print(f"Mean accuracy: {stability['mean_accuracy']:.3f}")
else:
    print("‚ö†Ô∏è HIGH VARIANCE - Model may be overfitting")
    print("Consider: More data, regularization, or simpler model")
```

**Benefits**:
- ‚úÖ Multiple test sets (robust validation)
- ‚úÖ Temporal awareness (embargo prevents leakage)
- ‚úÖ Stability testing (detects overfitting)
- ‚úÖ Time series aware (respects chronological order)

---

## üîß Integration Steps

### Step 1: Update `retrain_model.py`

**File**: `/root/AlgoTrendy_v2.6/retrain_model.py`

```python
# Add import at top of file
from MEM.cpcv_validator import (
    CombinatorialPurgedCrossValidator,
    CPCVConfig,
    AdaptiveCPCV  # Optional: Adapts embargo to market regime
)

# Replace existing validation code with CPCV
def train_with_cpcv(features_df, labels_series, timestamps_series):
    """
    Train model using CPCV for robust validation

    Args:
        features_df: DataFrame with calculated indicators
        labels_series: Series with reversal labels
        timestamps_series: Series with timestamps

    Returns:
        dict: Training results with stability metrics
    """
    # Configure CPCV
    config = CPCVConfig(
        n_splits=5,
        embargo_pct=0.01,  # 1% embargo (adjust based on your data frequency)
        test_size=0.2,
        min_train_size=0.5
    )

    # For adaptive embargo based on market regime, use:
    # validator = AdaptiveCPCV(config)
    validator = CombinatorialPurgedCrossValidator(config)

    # Initialize model (existing GradientBoostingClassifier)
    from sklearn.ensemble import GradientBoostingClassifier
    model = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )

    # Cross-validate
    fold_results = []
    for fold in validator.split(features_df, labels_series, timestamps_series):
        # Get fold data
        X_train = features_df.iloc[fold.train_indices]
        X_test = features_df.iloc[fold.test_indices]
        y_train = labels_series.iloc[fold.train_indices]
        y_test = labels_series.iloc[fold.test_indices]

        # Train model
        model.fit(X_train, y_train)

        # Evaluate
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        y_pred = model.predict(X_test)

        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0)
        }

        fold.performance_metrics = metrics
        fold_results.append(metrics)

        logger.info(
            f"Fold {fold.fold_id}: Accuracy={metrics['accuracy']:.3f}, "
            f"Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}"
        )

    # Calculate stability metrics
    stability = validator.calculate_stability_metrics()

    # Log results
    logger.info(
        f"\nCPCV Results:\n"
        f"  Mean Accuracy: {stability['mean_accuracy']:.3f}\n"
        f"  Std Accuracy: {stability['std_accuracy']:.3f}\n"
        f"  Coefficient of Variation: {stability['coefficient_of_variation']:.3f}\n"
        f"  Status: {'STABLE ‚úì' if stability['coefficient_of_variation'] < 0.15 else 'HIGH VARIANCE ‚ö†Ô∏è'}"
    )

    # Decision: Deploy if stable
    deploy = stability['coefficient_of_variation'] < 0.15 and stability['mean_accuracy'] > 0.70

    if deploy:
        # Train final model on ALL data
        model.fit(features_df, labels_series)

        # Save model
        import joblib
        model_path = f"ml_models/trend_reversals/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(model_path, exist_ok=True)
        joblib.dump(model, f"{model_path}/reversal_model.joblib")

        logger.info(f"‚úÖ Model deployed to {model_path}")
    else:
        logger.warning(
            "‚ö†Ô∏è Model NOT deployed due to high variance or low accuracy. "
            "Consider: More data, regularization, or feature engineering."
        )

    return {
        'deployed': deploy,
        'stability': stability,
        'fold_results': fold_results,
        'validator': validator
    }
```

### Step 2: Update Model Metrics Tracking

```python
# Save CPCV metrics for monitoring
def save_cpcv_metrics(results, model_path):
    """Save CPCV validation metrics alongside model"""
    metrics = {
        'validation_method': 'CPCV',
        'n_folds': len(results['fold_results']),
        'stability': results['stability'],
        'fold_results': results['fold_results'],
        'deployed': results['deployed'],
        'timestamp': datetime.now().isoformat()
    }

    with open(f"{model_path}/cpcv_metrics.json", 'w') as f:
        json.dump(metrics, f, indent=2)

    logger.info(f"CPCV metrics saved to {model_path}/cpcv_metrics.json")
```

### Step 3: Compare CPCV vs Current Method

```python
# Script to compare validation methods
def compare_validation_methods(features, labels, timestamps):
    """
    Compare simple train/test split vs CPCV
    """
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import GradientBoostingClassifier

    print("\n" + "="*80)
    print("VALIDATION METHOD COMPARISON")
    print("="*80)

    # Method 1: Simple Split (Current)
    print("\n1. Simple Train/Test Split (Current Method)")
    print("-" * 80)
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.2, random_state=42
    )

    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    simple_accuracy = model.score(X_test, y_test)

    print(f"Accuracy: {simple_accuracy:.3f}")
    print("‚ö†Ô∏è WARNING: Single test set - could be overfitted!")

    # Method 2: CPCV (New)
    print("\n2. Combinatorial Purged Cross-Validation (CPCV)")
    print("-" * 80)
    results = train_with_cpcv(features, labels, timestamps)

    print(f"Mean Accuracy: {results['stability']['mean_accuracy']:.3f}")
    print(f"Std Accuracy: {results['stability']['std_accuracy']:.3f}")
    print(f"Min Accuracy: {results['stability']['min_accuracy']:.3f}")
    print(f"Max Accuracy: {results['stability']['max_accuracy']:.3f}")
    print(f"Coefficient of Variation: {results['stability']['coefficient_of_variation']:.3f}")

    # Verdict
    print("\n" + "="*80)
    print("VERDICT")
    print("="*80)

    if results['stability']['coefficient_of_variation'] < 0.15:
        print("‚úÖ CPCV shows STABLE performance - Safe to deploy")
    else:
        print("‚ö†Ô∏è CPCV shows HIGH VARIANCE - Model may be overfitted")
        print("   Recommendation: Add regularization or collect more data")

    # Compare accuracies
    accuracy_diff = abs(simple_accuracy - results['stability']['mean_accuracy'])
    if accuracy_diff > 0.05:
        print(f"\n‚ö†Ô∏è Large difference between methods ({accuracy_diff:.3f})")
        print("   This suggests the simple split may have been lucky/unlucky")
        print("   CPCV provides more reliable estimate")

    return results
```

---

## üìà Expected Results

### Before CPCV (Simple Split)
```
Training Accuracy: 89%
Test Accuracy: 78%
Production Accuracy: 62% üò¢
Status: OVERFITTED
```

### After CPCV
```
Fold 1: 76%
Fold 2: 78%
Fold 3: 74%
Fold 4: 77%
Fold 5: 75%
Mean: 76% ¬± 1.5%
Production Accuracy: 75% üéâ
Status: STABLE
```

**Key Improvement**: Production performance matches validation (not degraded)

---

## üö® Warning Signs

### High Variance (Overfitting)
```python
stability = validator.calculate_stability_metrics()

if stability['coefficient_of_variation'] > 0.15:
    print("‚ö†Ô∏è OVERFITTING DETECTED")
    print("Possible fixes:")
    print("  1. Reduce model complexity (lower max_depth)")
    print("  2. Add regularization (increase min_samples_split)")
    print("  3. Collect more training data")
    print("  4. Simplify features (remove highly correlated ones)")
    print("  5. Increase embargo period (prevent leakage)")
```

### Large Train/Test Gap
```python
# If training accuracy much higher than CPCV accuracy
if train_accuracy - stability['mean_accuracy'] > 0.10:
    print("‚ö†Ô∏è Large train/test gap - Model memorizing training data")
    print("Recommendation: Add dropout, regularization, or early stopping")
```

---

## üéØ Next Steps

1. **This Week**:
   - [ ] Update `retrain_model.py` with CPCV
   - [ ] Run comparison test on historical data
   - [ ] Monitor stability metrics

2. **Next Week**:
   - [ ] Replace simple split with CPCV in production
   - [ ] Set up automated alerts for high variance
   - [ ] Track false discovery rate reduction

3. **Month 1**:
   - [ ] Compare production performance vs CPCV predictions
   - [ ] Fine-tune embargo period based on results
   - [ ] Document lessons learned

---

## üìö Additional Resources

- **CPCV Paper**: "Backtest Overfitting in the Machine Learning Era" (2024)
- **Code**: `/root/AlgoTrendy_v2.6/MEM/cpcv_validator.py`
- **Tests**: Run `python3 MEM/cpcv_validator.py` to see demo
- **Roadmap**: `/root/AlgoTrendy_v2.6/MEM/MEM_ENHANCEMENT_ROADMAP_2025.md`

---

## ‚úÖ Integration Checklist

- [ ] Install dependencies: `pandas`, `numpy`, `scikit-learn`
- [ ] Import CPCV in `retrain_model.py`
- [ ] Replace train_test_split with CPCV
- [ ] Add stability metrics logging
- [ ] Test on historical data (3-year dataset minimum)
- [ ] Set deployment thresholds (CV < 0.15, accuracy > 0.70)
- [ ] Monitor production performance vs CPCV predictions
- [ ] Document any issues or edge cases

---

**Status**: READY FOR INTEGRATION ‚úÖ
**Impact**: HIGH - Critical for production reliability
**Effort**: 2-3 hours integration + testing

**Questions?** Review `/root/AlgoTrendy_v2.6/MEM/MEM_ENHANCEMENT_ROADMAP_2025.md` for full context
